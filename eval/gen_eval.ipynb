{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e1cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"science\")\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64582f2",
   "metadata": {},
   "source": [
    "# Helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e2ac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (honesty_nppl.response != helpfulness_nppl.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8234f716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.70 ± 0.02 \t reward: 0.30 ± 1.21 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: 0.71 ± 0.02 \t reward: 0.41 ± 1.26 vs. -0.33 ± 1.06\n",
      "Context length 3 - win-rate: 0.72 ± 0.02 \t reward: 0.40 ± 1.33 vs. -0.33 ± 1.06\n",
      "Context length 5 - win-rate: 0.75 ± 0.02 \t reward: 0.50 ± 1.20 vs. -0.33 ± 1.06\n",
      "Context length 10 - win-rate: 0.78 ± 0.02 \t reward: 0.76 ± 1.21 vs. -0.33 ± 1.06\n",
      "\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.40 ± 0.02 \t reward: 0.05 ± 1.32 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: 0.37 ± 0.02 \t reward: -0.01 ± 1.27 vs. 0.59 ± 1.05\n",
      "Context length 3 - win-rate: 0.34 ± 0.02 \t reward: -0.03 ± 1.22 vs. 0.59 ± 1.05\n",
      "Context length 5 - win-rate: 0.32 ± 0.02 \t reward: -0.07 ± 1.19 vs. 0.59 ± 1.05\n",
      "Context length 10 - win-rate: 0.31 ± 0.02 \t reward: -0.15 ± 1.15 vs. 0.59 ± 1.05\n"
     ]
    }
   ],
   "source": [
    "helpfulness_btl = pd.read_json(\"../generations/helpfulness/simple/test_generations_II.jsonl\", lines=True)\n",
    "helpfulness_nppl = pd.read_json(\"../generations/helpfulness/conditional/test_generations_II.jsonl\", lines=True)\n",
    "\n",
    "helpfulness_btl_filtered = helpfulness_btl[helpfulness_nppl.id.max() >= helpfulness_btl.id]\n",
    "\n",
    "df = helpfulness_nppl.merge(helpfulness_btl_filtered, on=[\"id\"], how=\"left\", suffixes=(\"_nppl\", \"_btl\")).reset_index()\n",
    "#df = df[mask]\n",
    "\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df[df.context_len_nppl == i].helpfulness_score_nppl >= df[df.context_len_nppl == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df[df.context_len_nppl == i].helpfulness_score_nppl.mean()\n",
    "    reward_std = df[df.context_len_nppl == i].helpfulness_score_nppl.std()\n",
    "    reward_mean_btl = df[df.context_len_nppl == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df[df.context_len_nppl == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "print(\"\\n\")  \n",
    "print(\"Honesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df[df.context_len_nppl == i].honesty_score_nppl >= df[df.context_len_nppl == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df[df.context_len_nppl == i].honesty_score_nppl.mean()\n",
    "    reward_std = df[df.context_len_nppl == i].honesty_score_nppl.std()\n",
    "    reward_mean_btl = df[df.context_len_nppl == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df[df.context_len_nppl == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e74d6",
   "metadata": {},
   "source": [
    "# Honesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12487b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.70 ± 0.02 \t reward: 0.30 ± 1.21 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: 0.68 ± 0.02 \t reward: 0.29 ± 1.28 vs. -0.33 ± 1.06\n",
      "Context length 3 - win-rate: 0.73 ± 0.02 \t reward: 0.42 ± 1.25 vs. -0.33 ± 1.06\n",
      "Context length 5 - win-rate: 0.77 ± 0.02 \t reward: 0.55 ± 1.20 vs. -0.33 ± 1.06\n",
      "Context length 10 - win-rate: 0.79 ± 0.02 \t reward: 0.85 ± 1.20 vs. -0.33 ± 1.06\n",
      "\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.40 ± 0.02 \t reward: 0.05 ± 1.32 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: 0.40 ± 0.02 \t reward: 0.16 ± 1.25 vs. 0.59 ± 1.05\n",
      "Context length 3 - win-rate: 0.33 ± 0.02 \t reward: -0.01 ± 1.29 vs. 0.59 ± 1.05\n",
      "Context length 5 - win-rate: 0.32 ± 0.02 \t reward: -0.12 ± 1.33 vs. 0.59 ± 1.05\n",
      "Context length 10 - win-rate: 0.29 ± 0.02 \t reward: -0.20 ± 1.11 vs. 0.59 ± 1.05\n"
     ]
    }
   ],
   "source": [
    "honesty_btl = pd.read_json(\"../generations/helpfulness/simple/test_generations_II.jsonl\", lines=True)\n",
    "honesty_nppl = pd.read_json(\"../generations/honesty/conditional/test_generations_II.jsonl\", lines=True)\n",
    "\n",
    "honesty_btl_filtered = honesty_btl[honesty_nppl.id.max() >= honesty_btl.id]\n",
    "\n",
    "df_2 = honesty_nppl.merge(honesty_btl_filtered, on=[\"id\"], how=\"left\", suffixes=(\"_nppl\", \"_btl\")).reset_index()\n",
    "#df_2 = df_2[mask]\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_2[df_2.context_len_nppl == i].helpfulness_score_nppl >= df_2[df_2.context_len_nppl == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_2[df_2.context_len_nppl == i].helpfulness_score_nppl.mean()\n",
    "    reward_std = df_2[df_2.context_len_nppl == i].helpfulness_score_nppl.std()\n",
    "    reward_mean_btl = df_2[df_2.context_len_nppl == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_2[df_2.context_len_nppl == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "print(\"\\n\") \n",
    "print(\"Honesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_2[df_2.context_len_nppl == i].honesty_score_nppl >= df_2[df_2.context_len_nppl == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_2[df_2.context_len_nppl == i].honesty_score_nppl.mean()\n",
    "    reward_std = df_2[df_2.context_len_nppl == i].honesty_score_nppl.std()\n",
    "    reward_mean_btl = df_2[df_2.context_len_nppl == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_2[df_2.context_len_nppl == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796640c",
   "metadata": {},
   "source": [
    "# Gold Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796d6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.64 ± 0.02 \t reward: 0.15 ± 1.25 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.39 ± 0.02 \t reward: 0.19 ± 1.24 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n"
     ]
    }
   ],
   "source": [
    "helpfulness_btl_golden = pd.read_json(\"../generations/helpfulness/simple_gold/test_generations_II.jsonl\", lines=True)\n",
    "df_3 = helpfulness_btl_golden.merge(helpfulness_btl, on=[\"id\"], how=\"left\", suffixes=(\"_golden\", \"_btl\")).reset_index()\n",
    "\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].helpfulness_score_golden >= df_3[df_3.context_len_golden == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].helpfulness_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].helpfulness_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"\\nHonesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].honesty_score_golden >= df_3[df_3.context_len_golden == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].honesty_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].honesty_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab6d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.53 ± 0.02 \t reward: -0.37 ± 1.14 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.59 ± 0.02 \t reward: 0.81 ± 1.08 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n"
     ]
    }
   ],
   "source": [
    "honesty_btl_golden = pd.read_json(\"../generations/honesty/simple_gold/test_generations_II.jsonl\", lines=True)\n",
    "df_3 = honesty_btl_golden.merge(honesty_btl, on=[\"id\"], how=\"left\", suffixes=(\"_golden\", \"_btl\")).reset_index()\n",
    "\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].helpfulness_score_golden >= df_3[df_3.context_len_golden == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].helpfulness_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].helpfulness_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"\\nHonesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].honesty_score_golden >= df_3[df_3.context_len_golden == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].honesty_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].honesty_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd929ba",
   "metadata": {},
   "source": [
    "# (In)Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e10b5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_mix = honesty_nppl.merge(helpfulness_nppl, on=[\"id\", \"context_len\", \"prompt\"], how=\"inner\", suffixes=(\"_honest\", \"_helpfulness\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54a36701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context_len\n",
       "10    343\n",
       "3     320\n",
       "5     311\n",
       "1     229\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mix[(honesty_nppl.response != helpfulness_nppl.response)].value_counts(\"context_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a4162ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2728"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mix[(honesty_nppl.response != helpfulness_nppl.response) & (honesty_nppl.context_len == 0)].id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff34c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
