{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e1cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(\"science\")\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64582f2",
   "metadata": {},
   "source": [
    "# Helfpulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8234f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpfulness_btl = pd.read_json(\"../generations/helpfulness/simple/test_generations.jsonl\", lines=True)\n",
    "helpfulness_nppl = pd.read_json(\"../generations/helpfulness/conditional/test_generations.jsonl\", lines=True)\n",
    "\n",
    "helpfulness_btl = helpfulness_btl[helpfulness_nppl.id.max() >= helpfulness_btl.id]\n",
    "\n",
    "df = helpfulness_nppl.merge(helpfulness_btl, on=[\"id\"], how=\"left\", suffixes=(\"_nppl\", \"_btl\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fed6130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.66 ± 0.02 \t reward: 0.27 ± 1.29 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: 0.69 ± 0.02 \t reward: 0.36 ± 1.35 vs. -0.33 ± 1.06\n",
      "Context length 3 - win-rate: 0.70 ± 0.02 \t reward: 0.35 ± 1.30 vs. -0.33 ± 1.06\n",
      "Context length 5 - win-rate: 0.70 ± 0.02 \t reward: 0.30 ± 1.31 vs. -0.33 ± 1.06\n",
      "Context length 10 - win-rate: 0.67 ± 0.02 \t reward: 0.28 ± 1.30 vs. -0.33 ± 1.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df[df.context_len_nppl == i].helpfulness_score_nppl >= df[df.context_len_nppl == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df[df.context_len_nppl == i].helpfulness_score_nppl.mean()\n",
    "    reward_std = df[df.context_len_nppl == i].helpfulness_score_nppl.std()\n",
    "    reward_mean_btl = df[df.context_len_nppl == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df[df.context_len_nppl == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8743a3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honesty\n",
      "Context length 0 - win-rate: 0.42 ± 0.02 \t reward: 0.14 ± 1.32 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: 0.39 ± 0.02 \t reward: 0.11 ± 1.26 vs. 0.59 ± 1.05\n",
      "Context length 3 - win-rate: 0.37 ± 0.02 \t reward: 0.07 ± 1.30 vs. 0.59 ± 1.05\n",
      "Context length 5 - win-rate: 0.38 ± 0.02 \t reward: 0.10 ± 1.26 vs. 0.59 ± 1.05\n",
      "Context length 10 - win-rate: 0.39 ± 0.02 \t reward: 0.20 ± 1.25 vs. 0.59 ± 1.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Honesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df[df.context_len_nppl == i].honesty_score_nppl >= df[df.context_len_nppl == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df[df.context_len_nppl == i].honesty_score_nppl.mean()\n",
    "    reward_std = df[df.context_len_nppl == i].honesty_score_nppl.std()\n",
    "    reward_mean_btl = df[df.context_len_nppl == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df[df.context_len_nppl == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e74d6",
   "metadata": {},
   "source": [
    "# Honesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12487b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_btl = pd.read_json(\"../generations/helpfulness/simple/test_generations.jsonl\", lines=True)\n",
    "honesty_nppl = pd.read_json(\"../generations/honesty/conditional/test_generations.jsonl\", lines=True)\n",
    "\n",
    "honesty_btl = honesty_btl[honesty_nppl.id.max() >= honesty_btl.id]\n",
    "\n",
    "df_2 = honesty_nppl.merge(honesty_btl, on=[\"id\"], how=\"left\", suffixes=(\"_nppl\", \"_btl\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4b8dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.66 ± 0.02 \t reward: 0.27 ± 1.29 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: 0.69 ± 0.02 \t reward: 0.36 ± 1.35 vs. -0.33 ± 1.06\n",
      "Context length 3 - win-rate: 0.70 ± 0.02 \t reward: 0.35 ± 1.30 vs. -0.33 ± 1.06\n",
      "Context length 5 - win-rate: 0.70 ± 0.02 \t reward: 0.30 ± 1.31 vs. -0.33 ± 1.06\n",
      "Context length 10 - win-rate: 0.67 ± 0.02 \t reward: 0.28 ± 1.30 vs. -0.33 ± 1.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_2[df_2.context_len_nppl == i].helpfulness_score_nppl >= df_2[df_2.context_len_nppl == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_2[df_2.context_len_nppl == i].helpfulness_score_nppl.mean()\n",
    "    reward_std = df_2[df_2.context_len_nppl == i].helpfulness_score_nppl.std()\n",
    "    reward_mean_btl = df_2[df_2.context_len_nppl == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_2[df_2.context_len_nppl == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f4dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honesty\n",
      "Context length 0 - win-rate: 0.42 ± 0.02 \t reward: 0.14 ± 1.32 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: 0.39 ± 0.02 \t reward: 0.11 ± 1.26 vs. 0.59 ± 1.05\n",
      "Context length 3 - win-rate: 0.37 ± 0.02 \t reward: 0.07 ± 1.30 vs. 0.59 ± 1.05\n",
      "Context length 5 - win-rate: 0.38 ± 0.02 \t reward: 0.10 ± 1.26 vs. 0.59 ± 1.05\n",
      "Context length 10 - win-rate: 0.39 ± 0.02 \t reward: 0.20 ± 1.25 vs. 0.59 ± 1.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Honesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_2[df_2.context_len_nppl == i].honesty_score_nppl >= df_2[df_2.context_len_nppl == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_2[df_2.context_len_nppl == i].honesty_score_nppl.mean()\n",
    "    reward_std = df_2[df_2.context_len_nppl == i].honesty_score_nppl.std()\n",
    "    reward_mean_btl = df_2[df_2.context_len_nppl == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_2[df_2.context_len_nppl == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdd40b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(helpfulness_nppl.honesty_score == honesty_nppl.honesty_score).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b30337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Certainly, I'd be happy to help! Can you provi...\n",
       "1       To get high-grade support specifically for lan...\n",
       "2       To solve this problem, we'll follow these step...\n",
       "3       HYPOTHETICAL\\n\\nExplanation:\\nThe provided ane...\n",
       "4       No\\n\\nExplanation:\\nThe Head event \"PersonX be...\n",
       "                              ...                        \n",
       "2525    The American Chemical Society (ACS) has establ...\n",
       "2526    The length of time required for handcrafting t...\n",
       "2527    English\\n\\n**Rationale:** \\n- The given text i...\n",
       "2528    Midwest Africa has been experiencing an increa...\n",
       "2529    En aquesta trobada, els comites han expressat ...\n",
       "Name: response, Length: 2530, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpfulness_nppl.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a6924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Certainly, I'd be happy to help! Can you provi...\n",
       "1       To get high-grade support specifically for lan...\n",
       "2       To solve this problem, we'll follow these step...\n",
       "3       HYPOTHETICAL\\n\\nExplanation:\\nThe provided ane...\n",
       "4       No\\n\\nExplanation:\\nThe Head event \"PersonX be...\n",
       "                              ...                        \n",
       "2525    The American Chemical Society (ACS) has establ...\n",
       "2526    The length of time required for handcrafting t...\n",
       "2527    English\\n\\n**Rationale:** \\n- The given text i...\n",
       "2528    Midwest Africa has been experiencing an increa...\n",
       "2529    En aquesta trobada, els comites han expressat ...\n",
       "Name: response, Length: 2530, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honesty_nppl.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796640c",
   "metadata": {},
   "source": [
    "# Gold Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796d6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.64 ± 0.02 \t reward: 0.15 ± 1.25 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.39 ± 0.02 \t reward: 0.19 ± 1.24 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n"
     ]
    }
   ],
   "source": [
    "helpfulness_btl_golden = pd.read_json(\"../generations/helpfulness/simple_gold/test_generations.jsonl\", lines=True)\n",
    "df_3 = helpfulness_btl_golden.merge(helpfulness_btl, on=[\"id\"], how=\"left\", suffixes=(\"_golden\", \"_btl\")).reset_index()\n",
    "\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].helpfulness_score_golden >= df_3[df_3.context_len_golden == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].helpfulness_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].helpfulness_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"\\nHonesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].honesty_score_golden >= df_3[df_3.context_len_golden == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].honesty_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].honesty_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab6d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpfulness\n",
      "Context length 0 - win-rate: 0.53 ± 0.02 \t reward: -0.37 ± 1.14 vs. -0.33 ± 1.06\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "\n",
      "Honesty\n",
      "Context length 0 - win-rate: 0.59 ± 0.02 \t reward: 0.81 ± 1.08 vs. 0.59 ± 1.05\n",
      "Context length 1 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 3 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 5 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n",
      "Context length 10 - win-rate: nan ± nan \t reward: nan ± nan vs. nan ± nan\n"
     ]
    }
   ],
   "source": [
    "honesty_btl_golden = pd.read_json(\"../generations/honesty/simple_gold/test_generations.jsonl\", lines=True)\n",
    "df_3 = honesty_btl_golden.merge(honesty_btl, on=[\"id\"], how=\"left\", suffixes=(\"_golden\", \"_btl\")).reset_index()\n",
    "\n",
    "\n",
    "print(\"Helpfulness\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].helpfulness_score_golden >= df_3[df_3.context_len_golden == i].helpfulness_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].helpfulness_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].helpfulness_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].helpfulness_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"\\nHonesty\")\n",
    "for i in [0,1,3,5,10]:\n",
    "    comparison = (df_3[df_3.context_len_golden == i].honesty_score_golden >= df_3[df_3.context_len_golden == i].honesty_score_btl)\n",
    "    mean = comparison.mean()\n",
    "    n = len(comparison)\n",
    "    se = comparison.std() / np.sqrt(n)\n",
    "    reward_mean = df_3[df_3.context_len_golden == i].honesty_score_golden.mean()\n",
    "    reward_std = df_3[df_3.context_len_golden == i].honesty_score_golden.std()\n",
    "    reward_mean_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.mean()\n",
    "    reward_std_btl = df_3[df_3.context_len_golden == i].honesty_score_btl.std()\n",
    "    print(f\"Context length {i} - win-rate: {mean:.2f} ± {se:.2f} \\t reward: {reward_mean:.2f} ± {reward_std:.2f} vs. {reward_mean_btl:.2f} ± {reward_std_btl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd929ba",
   "metadata": {},
   "source": [
    "# (In)Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10b5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_honest = pd.read_json(\"../generations/honesty/simple/test_generations_save.jsonl\", lines=True)\n",
    "test_helpfulness = pd.read_json(\"../generations/helpfulness/simple/test_generations_save.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a36701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_honest.honesty_score == test_helpfulness.honesty_score).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d47cc3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_honest.helpfulness_score == test_helpfulness.helpfulness_score).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bb126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
