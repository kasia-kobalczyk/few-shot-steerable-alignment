{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "../saves/reward-models-ultrafeedback/hht-btl-mixed_0\n",
      "../saves/reward-models-ultrafeedback/hht-btl-honesty_0\n",
      "../saves/reward-models-ultrafeedback/hht-btl-helpfulness_0\n",
      "../saves/reward-models-ultrafeedback/hht-btl-truthfulness_0\n",
      "../saves/reward-models-ultrafeedback/hht-nppl-mixed_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pdata/knk25/cPL/eval/../eval/utils.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f'{save_dir}/model_{load_it}.pt', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from data.utils import setup_dataloaders, collect_pairs_choices, get_dataloader\n",
    "from data.dataloaders import ConflictingDataset\n",
    "from eval.utils import load_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device='cuda:0'\n",
    "\n",
    "root = '../saves/reward-models-ultrafeedback'\n",
    "\n",
    "# Load the models for the HHT datasets\n",
    "save_dict = {\n",
    "    'btl-mixed' : f'{root}/hht-btl-mixed_0',\n",
    "    'btl-honesty': f'{root}/hht-btl-honesty_0',\n",
    "    'btl-helpfulness': f'{root}/hht-btl-helpfulness_0',\n",
    "    'btl-truthfulness': f'{root}/hht-btl-truthfulness_0',\n",
    "    'nppl-mixed' : f'{root}/hht-nppl-mixed_0'\n",
    "}\n",
    "\n",
    "# # Load the models for the HH datasets\n",
    "# save_dict = {\n",
    "#     'btl-mixed' : f'{root}/hh-btl-mixed_0',\n",
    "#     'btl-honesty': f'{root}/hh-btl-honesty_0',\n",
    "#     'btl-helpfulness': f'{root}/hh-btl-helpfulness_0',\n",
    "#     'nppl-mixed' : f'{root}/hh-nppl-mixed_0'\n",
    "# }\n",
    "\n",
    "models = {}\n",
    "cfgs = {}\n",
    "for k, v in save_dict.items():\n",
    "    model, cfg = load_model(v, load_it='best', device=device)\n",
    "    models[k] = model\n",
    "    cfgs[k] = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d79c4071674c5dafc574d93e878199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134b3358dfe54b7fa102916425904392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting model: btl-mixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting model: btl-honesty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting model: btl-helpfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting model: btl-truthfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting model: nppl-mixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfg = cfgs['nppl-mixed']\n",
    "\n",
    "test_data_loader = setup_dataloaders(cfg.data, splits=['test'])['test']\n",
    "\n",
    "num_context_ls = [0, 1, 3, 5, 10]\n",
    "\n",
    "eval_dict = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    eval_dict[model_name] = {}\n",
    "    for metric in ['accuracy', 'unseen_accuracy', 'label']:\n",
    "        eval_dict[model_name][metric] = {}\n",
    "        for num_context in num_context_ls:\n",
    "            eval_dict[model_name][metric][num_context] = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print('Evaluting model:', model_name)\n",
    "    for i, batch in enumerate(tqdm(test_data_loader)):\n",
    "        for num_context in num_context_ls:\n",
    "            \n",
    "            pairs_C, choices_C, pairs_T, choices_T = collect_pairs_choices(\n",
    "                batch, \n",
    "                num_context=num_context,\n",
    "                min_num_context=cfg.data.min_num_context,\n",
    "                max_num_context=cfg.data.max_num_context,\n",
    "                num_targets=cfg.data.num_targets,\n",
    "                context_datatype=cfg.data.context_datatype\n",
    "            )  \n",
    "\n",
    "            pairs_T = pairs_T.to(device)\n",
    "            choices_T = choices_T.to(device)\n",
    "            pairs_C = pairs_C.to(device)\n",
    "            choices_C = choices_C.to(device) \n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(pairs_T, choices_T, pairs_C, choices_C)\n",
    "            \n",
    "            predictions = outputs['logp_choices'].argmax(dim=-1).unsqueeze(-1)\n",
    "            choices = choices_T.unsqueeze(0).expand(predictions.shape[0], -1, -1, -1)\n",
    "            acc = (predictions == choices).float().mean(axis=0)\n",
    "\n",
    "            bs = cfg.data.batch_size\n",
    "            num_targets = cfg.data.num_targets\n",
    "\n",
    "            unseen_predictions = torch.zeros(\n",
    "                (predictions.shape[0], bs, num_targets - num_context, 1)\n",
    "            )\n",
    "            unseen_choices = torch.zeros(\n",
    "                (predictions.shape[0], bs, num_targets - num_context, 1)\n",
    "            )\n",
    "\n",
    "            for i in range(bs):\n",
    "                idx = torch.tensor(list(range(num_context, num_targets)))\n",
    "                unseen_predictions[:, i, :, :] = predictions[:, i, idx, :]\n",
    "                unseen_choices[:, i, :, :] = choices[:, i, idx, :]\n",
    "\n",
    "            unseen_acc = (unseen_predictions == unseen_choices).float().mean(axis=0)\n",
    "        \n",
    "            eval_dict[model_name]['accuracy'][num_context].append(acc)\n",
    "            eval_dict[model_name]['unseen_accuracy'][num_context].append(unseen_acc)\n",
    "            eval_dict[model_name]['label'][num_context].append(batch['labels_T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">btl-helpfulness</th>\n",
       "      <th colspan=\"3\" halign=\"left\">btl-honesty</th>\n",
       "      <th colspan=\"3\" halign=\"left\">btl-mixed</th>\n",
       "      <th colspan=\"3\" halign=\"left\">btl-truthfulness</th>\n",
       "      <th colspan=\"3\" halign=\"left\">nppl-mixed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>honesty</th>\n",
       "      <th>truthfulness</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>honesty</th>\n",
       "      <th>truthfulness</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>honesty</th>\n",
       "      <th>truthfulness</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>honesty</th>\n",
       "      <th>truthfulness</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>honesty</th>\n",
       "      <th>truthfulness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_context</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.6 ± 0.3</td>\n",
       "      <td>43.1 ± 0.3</td>\n",
       "      <td>30.3 ± 0.3</td>\n",
       "      <td>41.9 ± 0.3</td>\n",
       "      <td>61.1 ± 0.3</td>\n",
       "      <td>51.2 ± 0.3</td>\n",
       "      <td>50.6 ± 0.3</td>\n",
       "      <td>52.6 ± 0.3</td>\n",
       "      <td>54.2 ± 0.3</td>\n",
       "      <td>29.8 ± 0.3</td>\n",
       "      <td>53.0 ± 0.3</td>\n",
       "      <td>70.5 ± 0.3</td>\n",
       "      <td>70.4 ± 0.3</td>\n",
       "      <td>46.2 ± 0.3</td>\n",
       "      <td>33.8 ± 0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.6 ± 0.3</td>\n",
       "      <td>43.1 ± 0.3</td>\n",
       "      <td>30.3 ± 0.3</td>\n",
       "      <td>41.9 ± 0.3</td>\n",
       "      <td>61.1 ± 0.3</td>\n",
       "      <td>51.2 ± 0.3</td>\n",
       "      <td>50.6 ± 0.3</td>\n",
       "      <td>52.6 ± 0.3</td>\n",
       "      <td>54.2 ± 0.3</td>\n",
       "      <td>29.8 ± 0.3</td>\n",
       "      <td>53.0 ± 0.3</td>\n",
       "      <td>70.5 ± 0.3</td>\n",
       "      <td>71.9 ± 0.3</td>\n",
       "      <td>47.5 ± 0.4</td>\n",
       "      <td>35.6 ± 0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.6 ± 0.3</td>\n",
       "      <td>43.1 ± 0.3</td>\n",
       "      <td>30.3 ± 0.3</td>\n",
       "      <td>41.9 ± 0.3</td>\n",
       "      <td>61.1 ± 0.3</td>\n",
       "      <td>51.2 ± 0.3</td>\n",
       "      <td>50.6 ± 0.3</td>\n",
       "      <td>52.6 ± 0.3</td>\n",
       "      <td>54.2 ± 0.3</td>\n",
       "      <td>29.8 ± 0.3</td>\n",
       "      <td>53.0 ± 0.3</td>\n",
       "      <td>70.5 ± 0.3</td>\n",
       "      <td>73.2 ± 0.4</td>\n",
       "      <td>53.8 ± 0.4</td>\n",
       "      <td>53.2 ± 0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77.6 ± 0.3</td>\n",
       "      <td>43.1 ± 0.3</td>\n",
       "      <td>30.3 ± 0.3</td>\n",
       "      <td>41.9 ± 0.3</td>\n",
       "      <td>61.1 ± 0.3</td>\n",
       "      <td>51.2 ± 0.3</td>\n",
       "      <td>50.6 ± 0.3</td>\n",
       "      <td>52.6 ± 0.3</td>\n",
       "      <td>54.2 ± 0.3</td>\n",
       "      <td>29.8 ± 0.3</td>\n",
       "      <td>53.0 ± 0.3</td>\n",
       "      <td>70.5 ± 0.3</td>\n",
       "      <td>73.5 ± 0.4</td>\n",
       "      <td>57.5 ± 0.4</td>\n",
       "      <td>67.5 ± 0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>77.6 ± 0.3</td>\n",
       "      <td>43.1 ± 0.3</td>\n",
       "      <td>30.3 ± 0.3</td>\n",
       "      <td>41.9 ± 0.3</td>\n",
       "      <td>61.1 ± 0.3</td>\n",
       "      <td>51.2 ± 0.3</td>\n",
       "      <td>50.6 ± 0.3</td>\n",
       "      <td>52.6 ± 0.3</td>\n",
       "      <td>54.2 ± 0.3</td>\n",
       "      <td>29.8 ± 0.3</td>\n",
       "      <td>53.0 ± 0.3</td>\n",
       "      <td>70.5 ± 0.3</td>\n",
       "      <td>71.5 ± 0.5</td>\n",
       "      <td>60.4 ± 0.5</td>\n",
       "      <td>72.3 ± 0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model       btl-helpfulness                          btl-honesty              \\\n",
       "labels          helpfulness     honesty truthfulness helpfulness     honesty   \n",
       "num_context                                                                    \n",
       "0                77.6 ± 0.3  43.1 ± 0.3   30.3 ± 0.3  41.9 ± 0.3  61.1 ± 0.3   \n",
       "1                77.6 ± 0.3  43.1 ± 0.3   30.3 ± 0.3  41.9 ± 0.3  61.1 ± 0.3   \n",
       "3                77.6 ± 0.3  43.1 ± 0.3   30.3 ± 0.3  41.9 ± 0.3  61.1 ± 0.3   \n",
       "5                77.6 ± 0.3  43.1 ± 0.3   30.3 ± 0.3  41.9 ± 0.3  61.1 ± 0.3   \n",
       "10               77.6 ± 0.3  43.1 ± 0.3   30.3 ± 0.3  41.9 ± 0.3  61.1 ± 0.3   \n",
       "\n",
       "model                      btl-mixed                           \\\n",
       "labels      truthfulness helpfulness     honesty truthfulness   \n",
       "num_context                                                     \n",
       "0             51.2 ± 0.3  50.6 ± 0.3  52.6 ± 0.3   54.2 ± 0.3   \n",
       "1             51.2 ± 0.3  50.6 ± 0.3  52.6 ± 0.3   54.2 ± 0.3   \n",
       "3             51.2 ± 0.3  50.6 ± 0.3  52.6 ± 0.3   54.2 ± 0.3   \n",
       "5             51.2 ± 0.3  50.6 ± 0.3  52.6 ± 0.3   54.2 ± 0.3   \n",
       "10            51.2 ± 0.3  50.6 ± 0.3  52.6 ± 0.3   54.2 ± 0.3   \n",
       "\n",
       "model       btl-truthfulness                           nppl-mixed              \\\n",
       "labels           helpfulness     honesty truthfulness helpfulness     honesty   \n",
       "num_context                                                                     \n",
       "0                 29.8 ± 0.3  53.0 ± 0.3   70.5 ± 0.3  70.4 ± 0.3  46.2 ± 0.3   \n",
       "1                 29.8 ± 0.3  53.0 ± 0.3   70.5 ± 0.3  71.9 ± 0.3  47.5 ± 0.4   \n",
       "3                 29.8 ± 0.3  53.0 ± 0.3   70.5 ± 0.3  73.2 ± 0.4  53.8 ± 0.4   \n",
       "5                 29.8 ± 0.3  53.0 ± 0.3   70.5 ± 0.3  73.5 ± 0.4  57.5 ± 0.4   \n",
       "10                29.8 ± 0.3  53.0 ± 0.3   70.5 ± 0.3  71.5 ± 0.5  60.4 ± 0.5   \n",
       "\n",
       "model                     \n",
       "labels      truthfulness  \n",
       "num_context               \n",
       "0             33.8 ± 0.3  \n",
       "1             35.6 ± 0.3  \n",
       "3             53.2 ± 0.4  \n",
       "5             67.5 ± 0.4  \n",
       "10            72.3 ± 0.4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_context = 0\n",
    "\n",
    "def get_res_df(eval_dict, acc_type=\"accuracy\"):\n",
    "    acc_dict = eval_dict[acc_type]\n",
    "    labels_dict = eval_dict['label']\n",
    "    res_df = pd.DataFrame()\n",
    "    for num_context in num_context_ls:\n",
    "        mean_acc = torch.stack(acc_dict[num_context]).squeeze(-1).mean(dim=-1).cpu().numpy()\n",
    "        labels = [z for zs in labels_dict[num_context] for z in zs]\n",
    "        try:\n",
    "            res = pd.DataFrame({\n",
    "                'acc' : mean_acc.flatten() * 100,\n",
    "                'labels' : labels,\n",
    "                'num_context' : num_context\n",
    "            })\n",
    "        except(ValueError):\n",
    "            res = pd.DataFrame()\n",
    "            print(acc_type, num_context)\n",
    "        res_df = pd.concat([res_df, res])\n",
    "\n",
    "    return res_df\n",
    "\n",
    "res_df = pd.DataFrame()\n",
    "\n",
    "for model_name in models.keys():\n",
    "    res_df_model = get_res_df(eval_dict[model_name], acc_type=\"unseen_accuracy\" if 'nppl' in model_name else \"accuracy\")\n",
    "    res_df_model['model'] = model_name\n",
    "    res_df = pd.concat([res_df, res_df_model])\n",
    "\n",
    "\n",
    "summary_df = res_df.groupby(['model', 'num_context', 'labels'])['acc'].agg(['mean', 'sem']) \n",
    "summary_df['acc'] = summary_df.apply(lambda x: f'{x['mean']:.1f} ± {x['sem']:.1f}', axis=1)\n",
    "summary_df.reset_index().pivot(index='num_context', columns=['model', 'labels'], values='acc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
