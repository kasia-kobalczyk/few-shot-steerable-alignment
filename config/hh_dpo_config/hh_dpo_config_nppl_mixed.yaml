model:
  model_type: 'policy'
  device: 'cuda:0'
  input_dim: 4096
  hidden_dim: 256
  llm_name: 'Qwen/Qwen2.5-1.5B-Instruct'
  tune_llm: True
  lora_alpha: 256
  lora_dropout: 0.1
  lora_r: 128
  num_film_layers: -1
  # NPPL
  is_conditional: True
  sample_latents: False
  context_agg_func: 'self-attention'
  context_input_encoder: 'linear'
loss:
  beta_dpo: 0.05
training:
  num_epochs: 1
  lr: 1e-5
  seed: 0
  dry_run: False
data:
  min_num_context: 0
  max_num_context: 6
  num_targets: 9
  batch_size: 2
  context_datatype: 'embeddings'
  target_datatype: 'tokens'
  path_to_context_data: '/mnt/pdata/knk25/cPL/data/ultra_feedback/embedded_pairs_1024/meta-llama/Meta-Llama-3-8B'
  path_to_target_data: '/mnt/pdata/caf83/few-shot-alignment/data/ultra_feedback/tokenized_pairs_512/Qwen/Qwen2.5-1.5B-Instruct'
  split_file: '/mnt/pdata/knk25/cPL/data/ultra_feedback/hh_pairs_conflict_1.0.csv'
  labels: ['helpfulness', 'honesty']
save:
  project_name: 'dpo-ultrafeedback'
  run_name_prefix: 'hh-dpo-nppl-mixed-mega'