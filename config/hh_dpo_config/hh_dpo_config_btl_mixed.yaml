model:
  model_type: 'policy'
  device: 'cuda:0'
  input_dim: 4096
  hidden_dim: 256
  llm_name: 'google/gemma-2b'
  tune_llm: True
  lora_alpha: 1024
  lora_dropout: 0.1
  lora_r: 512
  num_film_layers: -1
  is_conditional: False
loss:
  beta_dpo: 0.05
training:
  num_epochs: 1
  lr: 1e-6
  seed: 0
  dry_run: False
data:
  min_num_context: 0
  max_num_context: 6
  num_targets: 9
  batch_size: 1
  context_datatype: 'embeddings'
  target_datatype: 'tokens'
  path_to_context_data: 'data/ultra_feedback/embedded_pairs/meta-llama/Meta-Llama-3-8B'
  path_to_target_data: 'data/ultra_feedback/tokenized_pairs_512/Columbia-NLP/gemma-2b-zephyr-sft'
  split_file: 'data/ultra_feedback/hh_pairs_conflict_1.0.csv'
  labels: ['helpfulness', 'honesty']
save:
  project_name: 'dpo-ultrafeedback'
  run_name_prefix: 'hh-dpo-btl-mixed'